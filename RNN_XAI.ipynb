{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lime import lime_tabular\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPLE REGRESION CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar data set sintético\n",
    "n = 10000\n",
    "mu= 0\n",
    "sigma =1\n",
    "\n",
    "np.random.seed(0)\n",
    "x1 = np.random.normal(mu,sigma,n)\n",
    "np.random.seed(1)\n",
    "x2 = np.random.normal(mu,sigma,n)\n",
    "np.random.seed(2)\n",
    "x3 = np.random.normal(mu,sigma,n)\n",
    "x1 = x1.reshape(-1,1)\n",
    "x2 = x2.reshape(-1,1)\n",
    "x3 = x3.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(seq, n_timesteps):\n",
    "    N = len(seq) - n_timesteps - 1\n",
    "    nf = seq.shape[1]\n",
    "    if N <= 0:\n",
    "        raise ValueError('I need more data!')\n",
    "    new_seq = np.zeros((N, n_timesteps, nf))\n",
    "    for i in range(N):\n",
    "        new_seq[i, :, :] = seq[i:i+n_timesteps]\n",
    "    return new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for RNN\n",
    "N_TIMESTEPS = 3\n",
    "X1 = reshape_data(x1, n_timesteps=N_TIMESTEPS)\n",
    "X1 = X1[:,::-1,:]\n",
    "X2 = reshape_data(x2, n_timesteps=N_TIMESTEPS)\n",
    "X2 = X2[:,::-1,:]\n",
    "X3 = reshape_data(x3, n_timesteps=N_TIMESTEPS)\n",
    "X3 = X3[:,::-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X1[:,0]**2 + X2[:,1] - X3[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X1 X2 X3\n",
    "X = np.concatenate((X1,X2,X3), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the RNN model and training\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(20, activation='tanh', return_sequences=False, input_shape=(N_TIMESTEPS, X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Step 4: Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Step 5: Train the model\n",
    "model.fit(X, y, epochs=50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting values \n",
    "y_pred = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the true values line\n",
    "fig.add_trace(go.Scatter(x=list(range(y.shape[0])), y=y.squeeze(), mode='lines', name='True Values'))\n",
    "\n",
    "# Add the predicted values line\n",
    "fig.add_trace(go.Scatter(x=list(range(y.shape[0])), y=y_pred.squeeze(), mode='lines', name='Predicted Values', line=dict(dash='dash')))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='True vs Predicted Values',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Value'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivadas Parciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtencion de los pesos\n",
    "model_weights = model.get_weights()\n",
    "W1 =model_weights[0] #input to hidden\n",
    "U1=model_weights[1] #hidden to hidden\n",
    "b1=model_weights[2] #hidden to hidden\n",
    "W2=model_weights[3] #hidden to output\n",
    "b2= model_weights[4] #hidden to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_rnn(X, W1, U1, b1, W2, b2):\n",
    "    def stanh(x):\n",
    "        return np.diag(1 - np.tanh(x) ** 2)\n",
    "\n",
    "    # Forward pass\n",
    "    nsamples, timesteps, ninputs = X.shape\n",
    "\n",
    "\n",
    "    # List of every layer \n",
    "    Z = [\n",
    "        X, \n",
    "        np.empty((nsamples, timesteps, W1.shape[1])), \n",
    "        np.empty((nsamples, timesteps, 1))\n",
    "    ]\n",
    "    O = [\n",
    "        X, \n",
    "        np.empty((nsamples, timesteps, W1.shape[1])),\n",
    "        np.empty((nsamples, timesteps, 1))\n",
    "    ]\n",
    "    o_t1_l2 = np.zeros((nsamples, W1.shape[1]))\n",
    "\n",
    "    for t in range(timesteps):\n",
    "        # INPUT LAYER\n",
    "        z_t_l1 = Z[0][:, t, :].copy()\n",
    "        o_t_l1 = z_t_l1.copy()\n",
    "        \n",
    "        # HIDDEN LAYER\n",
    "        z_t_l2 = o_t_l1 @ W1 + o_t1_l2 @ U1 + b1\n",
    "        o_t_l2 = np.tanh(z_t_l2)\n",
    "        o_t1_l2 = o_t_l2.copy()\n",
    "        \n",
    "        # OUTPUT LAYER\n",
    "        z_t_l3 = o_t_l2 @ W2 + b2\n",
    "        o_t_l3 = z_t_l3\n",
    "        \n",
    "        #aquí solo te quedas con el ultimo timestep?\n",
    "        # STORE VARIABLES\n",
    "        Z[1][:, t, :] = z_t_l2\n",
    "        O[1][:, t, :] = o_t_l2\n",
    "        Z[2][:, t, :] = z_t_l3\n",
    "        O[2][:, t, :] = o_t_l3\n",
    "            \n",
    "    # backward pass\n",
    "    D=[np.eye(ninputs), np.apply_along_axis(stanh, 2, Z[1]), np.eye(1)]\n",
    "    \n",
    "    #dy_t_l3/dyz_t_l2\n",
    "    dydh  = D[1][:,-1,:,:] @ W2 @ D[2]\n",
    "\n",
    "    #dz_t_l2/dx_t_j\n",
    "    PD = np.empty((nsamples,timesteps,ninputs,y.shape[1]))\n",
    "    CD = np.eye(Z[1].shape[2])\n",
    "    for t in range (timesteps):\n",
    "        PD[:,t,:,:] = D[0] @ W1 @ CD @ dydh\n",
    "        CD = D[1][:,t,:,:] @ U1 @ CD\n",
    "\n",
    "    return PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PD = jacobian_rnn(X, W1, U1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sensitivities(PD):\n",
    "    # Sensibilidad media de la salida con respecto a la entrada i en el timestep t\n",
    "    mean_PD_it = np.mean(PD, axis=0)\n",
    "    \n",
    "    # Desviación estándar de la salida con respecto a la entrada i en el timestep t\n",
    "    sigma_PD_it = np.sqrt(np.mean((PD - mean_PD_it)**2, axis=0))\n",
    "    \n",
    "    # Sensibilidad cuadrática media de la salida con respecto a la entrada i en el timestep t\n",
    "    PD_it_squared = np.mean(PD**2, axis=0)\n",
    "    \n",
    "    # Sensibilidad media de la salida con respecto a la entrada i\n",
    "    mean_PD_i = np.mean(mean_PD_it, axis=0)\n",
    "    \n",
    "    # Desviación estándar de la salida con respecto a la entrada i\n",
    "    sigma_PD_i = np.sqrt(np.mean(PD_it_squared - mean_PD_it**2, axis=0))\n",
    "    \n",
    "    # Sensibilidad cuadrática media de la salida con respecto a la entrada i\n",
    "    PD_i_squared = np.mean(PD_it_squared, axis=0)\n",
    "    \n",
    "    return mean_PD_it, sigma_PD_it, PD_it_squared, mean_PD_i, sigma_PD_i, PD_i_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_PD_it, sigma_PD_it, PD_it_squared, mean_PD_i, sigma_PD_i, PD_i_squared = compute_sensitivities(PD)\n",
    "\n",
    "print(\"meanPD_it:\", mean_PD_it.shape)\n",
    "print(\"sigma_PD_it:\", sigma_PD_it.shape)\n",
    "print(\"PD_it_squared:\", PD_it_squared.shape)\n",
    "print(\"meanPD_i:\", mean_PD_i.shape)\n",
    "print(\"sigma_PD_i:\", sigma_PD_i.shape)\n",
    "print(\"PD_i_squared:\", PD_i_squared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_mean_PD = mean_PD_it.squeeze()\n",
    "flattened_sigma_PD_it = sigma_PD_it.squeeze()\n",
    "flattened_PD_it_squared = PD_it_squared.squeeze()\n",
    "# Crear un DataFrame con pandas y ajustar el índice y las columnas\n",
    "df_mean_PD = pd.DataFrame(flattened_mean_PD, index=['X1', 'X2', 'X3'], columns=['t', 't-1', 't-2'])\n",
    "df_sigma_PD_it = pd.DataFrame(flattened_sigma_PD_it, index=['X1', 'X2', 'X3'], columns=['t', 't-1', 't-2'])\n",
    "df_PD_it_squared= pd.DataFrame(flattened_PD_it_squared, index=['X1', 'X2', 'X3'], columns=['t', 't-1', 't-2'])\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df_mean_PD)\n",
    "print(df_sigma_PD_it)\n",
    "print(df_PD_it_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_partial_derivative_analysis(PD, mean_PD_it, sigma_PD_it, PD_it_squared, timestep_names=None):\n",
    "    \"\"\"\n",
    "    Generate various plots to analyze the relationship between output and inputs using partial derivatives.\n",
    "\n",
    "    Args:\n",
    "        PD (numpy.ndarray): Raw partial derivatives with size (samples, timesteps, inputs, outputs).\n",
    "        mean_PD_it (numpy.ndarray): Mean of partial derivatives in each timestep with size (timesteps, inputs, outputs).\n",
    "        sigma_PD_it (numpy.ndarray): Standard deviation of partial derivatives in each timestep with size (timesteps, inputs, outputs).\n",
    "        PD_it_squared (numpy.ndarray): Mean of squared partial derivatives in each timestep with size (timesteps, inputs, outputs).\n",
    "        timestep_names (list of str, optional): Names of the timesteps. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = mean_PD_it.shape[1]\n",
    "    timesteps = mean_PD_it.shape[0]\n",
    "    samples = PD.shape[0]\n",
    "    \n",
    "    input_names = [f'Input {i+1}' for i in range(inputs)]\n",
    "\n",
    "    if timestep_names is None:\n",
    "        timestep_names = [f't-{timesteps-t-1}' for t in range(timesteps)]\n",
    "\n",
    "    # Scatter plot: Mean vs Std of Partial Derivatives for Inputs Across All Timesteps\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.axhline(0, color='blue', linewidth=1)\n",
    "    plt.axvline(0, color='blue', linewidth=1)\n",
    "    plt.scatter(0, 0, color='blue', s=10) \n",
    "\n",
    "    for t in range(timesteps):\n",
    "        for i in range(inputs):\n",
    "            mean_val = mean_PD_it[t, i, 0]\n",
    "            std_val = sigma_PD_it[t, i, 0]\n",
    "            plt.scatter(mean_val, std_val, color='darkgray', label=f'{input_names[i]} ({timestep_names[t]})')\n",
    "            plt.text(mean_val, std_val, f'{input_names[i]} ({timestep_names[t]})', fontsize=8)\n",
    "\n",
    "    plt.xlabel('Mean of Partial Derivatives')\n",
    "    plt.ylabel('Standard Deviation of Partial Derivatives')\n",
    "    plt.title('Mean vs Standard Deviation of Partial Derivatives for Inputs Across All Timesteps')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Bar plot: Squared Partial Derivatives with Gradient Colors Based on Mean Values\n",
    "    flattened_data = []\n",
    "    for t in range(timesteps):\n",
    "        for i in range(inputs):\n",
    "            mean_val = mean_PD_it[t, i, 0]\n",
    "            squared_val = PD_it_squared[t, i, 0]\n",
    "            label = f'{input_names[i]} ({timestep_names[t]})'\n",
    "            flattened_data.append((mean_val, squared_val, label))\n",
    "\n",
    "    flattened_data.sort(key=lambda x: x[1])\n",
    "    sorted_means, sorted_squared_vals, labels = zip(*flattened_data)\n",
    "    cmap_red = plt.cm.Reds_r\n",
    "    cmap_green = plt.cm.Greens\n",
    "    cmap_gray = plt.cm.Greys\n",
    "    min_mean = np.min(sorted_means)\n",
    "    max_mean = np.max(sorted_means)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for mean_val, squared_val, label in zip(sorted_means, sorted_squared_vals, labels):\n",
    "        if mean_val < 0:\n",
    "            norm_val = (mean_val - min_mean) / (0 - min_mean)\n",
    "            color = cmap_red(norm_val)\n",
    "        elif mean_val > 0:\n",
    "            norm_val = (mean_val - 0) / (max_mean - 0)\n",
    "            color = cmap_green(norm_val)\n",
    "        else:\n",
    "            color = cmap_gray(0.5)\n",
    "\n",
    "        plt.bar(label, squared_val, color=color)\n",
    "\n",
    "    plt.xlabel('Inputs and Timesteps')\n",
    "    plt.ylabel('Squared Partial Derivatives')\n",
    "    plt.title('Squared Partial Derivatives with Gradient Colors Based on Mean Values')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Density plot: Partial Derivatives for Each Input and Timestep\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = sns.color_palette(\"husl\", timesteps * inputs)\n",
    "\n",
    "    for t in range(timesteps):\n",
    "        for i in range(inputs):\n",
    "            sns.kdeplot(PD[:, t, i, 0], fill=True, alpha=0.4, label=f'{input_names[i]}, {timestep_names[t]}')\n",
    "\n",
    "    plt.xlabel('Partial Derivatives')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Density Plots of Partial Derivatives for Each Input and Timestep')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Line plot: Evolution of Partial Derivatives Through Samples\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    colors = sns.color_palette(\"husl\", timesteps * inputs)\n",
    "\n",
    "    for t in range(timesteps):\n",
    "        for i in range(inputs):\n",
    "            plt.plot(range(samples), PD[:, t, i, 0], label=f'{input_names[i]}, {timestep_names[t]}', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Partial Derivatives')\n",
    "    plt.title('Evolution of Partial Derivatives Through Samples')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Subplots: Evolution of Mean, Std, and Squared Partial Derivatives\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 15), sharex=True)\n",
    "\n",
    "    axes[0].set_title('Evolution of the Mean of Partial Derivatives')\n",
    "    for i in range(inputs):\n",
    "        axes[0].plot(range(timesteps), mean_PD_it[:, i, 0], marker='o', label=f'Input {i+1}')\n",
    "    axes[0].set_ylabel('Mean of Partial Derivatives')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    axes[0].set_xticks(range(timesteps))\n",
    "    axes[0].set_xticklabels(timestep_names)\n",
    "\n",
    "    axes[1].set_title('Evolution of the Standard Deviation of Partial Derivatives')\n",
    "    for i in range(inputs):\n",
    "        axes[1].plot(range(timesteps), sigma_PD_it[:, i, 0], marker='o', label=f'Input {i+1}')\n",
    "    axes[1].set_ylabel('Standard Deviation of Partial Derivatives')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    axes[1].set_xticks(range(timesteps))\n",
    "    axes[1].set_xticklabels(timestep_names)\n",
    "\n",
    "    axes[2].set_title('Evolution of the Mean of Squared Partial Derivatives')\n",
    "    for i in range(inputs):\n",
    "        axes[2].plot(range(timesteps), PD_it_squared[:, i, 0], marker='o', label=f'Input {i+1}')\n",
    "    axes[2].set_xlabel('Timesteps')\n",
    "    axes[2].set_ylabel('Mean of Squared Partial Derivatives')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    axes[2].set_xticks(range(timesteps))\n",
    "    axes[2].set_xticklabels(timestep_names)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_partial_derivative_analysis(PD, mean_PD_it, sigma_PD_it, PD_it_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINE CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the function to generate sine wave data\n",
    "def generate_sin_wave(seq_length):\n",
    "    time_steps = np.linspace(0, 2 * np.pi, seq_length + 1)\n",
    "    data = np.sin(time_steps)\n",
    "    data.resize((seq_length + 1, 1))\n",
    "\n",
    "    x = data[:-1]\n",
    "    y = data[1:]\n",
    "    return time_steps, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate the data\n",
    "seq_length = 600  # You can change this value\n",
    "_, x, y = generate_sin_wave(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for RNN\n",
    "N_TIMESTEPS = 8\n",
    "x = reshape_data(x, n_timesteps=N_TIMESTEPS)\n",
    "y = y[N_TIMESTEPS:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create the RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(12, activation='tanh', return_sequences=False, input_shape=(N_TIMESTEPS, 1)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Step 4: Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Step 5: Train the model\n",
    "model.fit(x, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the true values line\n",
    "fig.add_trace(go.Scatter(x=list(range(y.shape[0])), y=y.squeeze(), mode='lines', name='True Values'))\n",
    "\n",
    "# Add the predicted values line\n",
    "fig.add_trace(go.Scatter(x=list(range(y.shape[0])), y=y_pred.squeeze(), mode='lines', name='Predicted Values', line=dict(dash='dash')))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='True vs Predicted Values',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Value'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y, lw=3, alpha=0.3, label='Truth')\n",
    "plt.plot(y_pred, '--', label='Predictions')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivadas Parciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtencion de los pesos\n",
    "model_weights = model.get_weights()\n",
    "W1 =model_weights[0] #input to hidden\n",
    "U1=model_weights[1] #hidden to hidden\n",
    "b1=model_weights[2] #hidden to hidden\n",
    "W2=model_weights[3] #hidden to output\n",
    "b2= model_weights[4] #hidden to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PD = jacobian_rnn(x, W1, U1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_PD_it, sigma_PD_it, PD_it_squared, mean_PD_i, sigma_PD_i, PD_i_squared = compute_sensitivities(PD)\n",
    "\n",
    "print(\"meanPD_it:\", mean_PD_it.shape)\n",
    "print(\"sigma_PD_it:\", sigma_PD_it.shape)\n",
    "print(\"PD_it_squared:\", PD_it_squared.shape)\n",
    "print(\"meanPD_i:\", mean_PD_i.shape)\n",
    "print(\"sigma_PD_i:\", sigma_PD_i.shape)\n",
    "print(\"PD_i_squared:\", PD_i_squared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_partial_derivative_analysis(PD, mean_PD_it, sigma_PD_it, PD_it_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_tabular.RecurrentTabularExplainer(training_data=x, \n",
    "                                                   training_labels=y, \n",
    "                                                   mode='regression',\n",
    "                                                   feature_names=['X1'],\n",
    "                                                   discretize_continuous=False,\n",
    "                                                   class_names=None, \n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_coefs = np.empty((591, 8))\n",
    "for p in range(x.shape[0]):\n",
    "    exp = explainer.explain_instance(x[p], model.predict, num_features=8, labels=None)\n",
    "    # exp.show_in_notebook()\n",
    "    lime_coefs[p,:] = np.array([i[1] for i in exp.as_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y, lw=3, alpha=0.3, label='Truth')\n",
    "plt.plot(lime_coefs[:,0], label='Xt')\n",
    "plt.plot(lime_coefs[:,1], '--', label='Xt-1')\n",
    "plt.plot(lime_coefs[:,2], '.-', label='Xt-2')\n",
    "plt.plot(lime_coefs[:,3], label='Xt-3')\n",
    "plt.plot(lime_coefs[:,4], '--', label='Xt-4')\n",
    "plt.plot(lime_coefs[:,5], '.-', label='Xt-5')\n",
    "plt.plot(lime_coefs[:,6], label='Xt-6')\n",
    "plt.plot(lime_coefs[:,7], '--', label='Xt-7')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(x[0], model.predict, num_features=8, labels=None)\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# Extracting the coefficients\n",
    "lime_coefficients = exp.as_list()\n",
    "\n",
    "# Printing the coefficients\n",
    "print(\"LIME Coefficients for the instance:\")\n",
    "for feature, weight in lime_coefficients:\n",
    "    print(f\"{feature}: {weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(x[75], model.predict, num_features=8, labels=None)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(x[150], model.predict, num_features=8, labels=None)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(x[225], model.predict, num_features=8, labels=None)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(x[300], model.predict, num_features=8, labels=None)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(x[375], model.predict, num_features=8, labels=None)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(x[450], model.predict, num_features=8, labels=None)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(x[525], model.predict, num_features=8, labels=None)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lime = 0.17 * x[:, 3, :] + 0.18 * x[:, 4, :] - 0.13 * x[:, 7, :] -0.12 * x[:, 5, :] - 0.03 * x[:, 6, :] + 0.27 * x[:, 0, :] + 0.42 * x[:, 2, :] - 0.20 * x[:, 1, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y, lw=3, alpha=0.3, label='Truth')\n",
    "plt.plot(y_pred, '--', label='Predictions')\n",
    "plt.plot(y_lime, '.-', label='Predictions LIME')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import seaborn as sns\n",
    "\n",
    "shap.initjs()\n",
    "x = np.array(x)\n",
    "explainer = shap.DeepExplainer(model, x)\n",
    "\n",
    "shap_values = explainer.shap_values(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values=np.squeeze(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "shap_values_array = np.array(shap_values[0])\n",
    "print(\"Shape of shap_values_array:\", shap_values_array.shape)\n",
    "shap_pd_list = []\n",
    "x_pd_list = []\n",
    "names_features=['X1']\n",
    "timestep_labels=['t','t-1','t-2','t-3','t-4','t-5','t-6','t-7']\n",
    "for i in range(shap_values_array.shape[2]):\n",
    "    shap_pd_list.append(pd.DataFrame(shap_values_array[:,:,i], columns=[str(names_features[i]) + '_' + str(timestep_labels[j]) for j in range(N_TIMESTEPS)]))\n",
    "    x_pd_list.append(pd.DataFrame(x[:,:,i], columns=[str(names_features[i]) + '_' + str(timestep_labels[j]) for j in range(N_TIMESTEPS)]))\n",
    "shap_pd = pd.concat(shap_pd_list, axis=1)\n",
    "x_pd = pd.concat(x_pd_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_pd.values, x_pd)\n",
    "shap.summary_plot(shap_pd.values, x_pd, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def PDP(model, data, time_step, feature_index, num_grid_points=50):\n",
    "    \"\"\"\n",
    "    Generates Partial Dependence Plot (PDP) and Individual Conditional Expectation (ICE) plots\n",
    "    for a given feature at a specific time step.\n",
    "\n",
    "    :param model: Trained Keras RNN model.\n",
    "    :param data: Input data (3D array: [batch_size, time_steps, features]).\n",
    "    :param time_step: Time step at which the PDP is to be generated.\n",
    "    :param feature_index: Index of the feature for which PDP is to be generated.\n",
    "    :param num_grid_points: Number of points for plotting.\n",
    "    \"\"\"\n",
    "    # Ensure the specified time step is within the range of the data's time steps\n",
    "    assert 0 <= time_step < data.shape[1], \"time_step is out of bounds.\"\n",
    "\n",
    "    # Get range for the feature\n",
    "    feature_range = np.linspace(np.min(data[:, time_step, feature_index]), \n",
    "                                np.max(data[:, time_step, feature_index]), \n",
    "                                num_grid_points)\n",
    "    \n",
    "    # Storage for model ice\n",
    "    ice = np.zeros((data.shape[0], len(feature_range)))\n",
    "\n",
    "    # Calculate ice for each value in feature range\n",
    "    for i, value in enumerate(feature_range):\n",
    "        modified_data = data.copy()\n",
    "        modified_data[:, time_step, feature_index] = value\n",
    "        ice[:, i] = model.predict(modified_data).flatten()\n",
    "\n",
    "    # Calculate PDP\n",
    "    pdp = ice.mean(axis=0)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # ICE Plots\n",
    "    for i in range(data.shape[0]):\n",
    "        plt.plot(feature_range, ice[i, :], color='grey', alpha=0.5)\n",
    "\n",
    "    # PDP\n",
    "    plt.plot(feature_range, pdp, color='red', linewidth=3, label='PDP')\n",
    "\n",
    "    plt.xlabel(f'Feature {names_features[feature_index]} at Time Step {timestep_labels[time_step]}')\n",
    "    plt.ylabel('Model Response')\n",
    "    plt.title(f'ICE and PDP for Feature {names_features[feature_index]} at Time Step {timestep_labels[time_step]}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return pdp, ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_PDPs(model, data, num_grid_points=50):\n",
    "    \"\"\"\n",
    "    Generates PDPs for each feature at each time step for the given RNN model.\n",
    "\n",
    "    :param model: Trained Keras RNN model.\n",
    "    :param data: Input data (3D array: [batch_size, time_steps, features]).\n",
    "    :param num_grid_points: Number of points for plotting.\n",
    "    \"\"\"\n",
    "    num_time_steps = data.shape[1]\n",
    "    print(num_time_steps)\n",
    "    num_features = data.shape[2]\n",
    "\n",
    "    pdp_results = {}\n",
    "\n",
    "    for time_step in range(num_time_steps):\n",
    "        for feature_index in range(num_features):\n",
    "            pdp = PDP(model, data, time_step, feature_index, num_grid_points)\n",
    "            pdp_results[(time_step, feature_index)] = pdp\n",
    "\n",
    "    return pdp_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_all_PDPs(model, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error  # or use another appropriate metric depending on your task\n",
    "\n",
    "def permutation_importance(model, X, y, metric=mean_squared_error):\n",
    "    \"\"\"\n",
    "    Compute permutation feature importance for each time step for each feature \n",
    "    in a Keras RNN model.\n",
    "\n",
    "    :param model: Trained Keras RNN model.\n",
    "    :param X: Input data (3D array: [batch_size, time_steps, features]).\n",
    "    :param y: True output values.\n",
    "    :param metric: Performance metric function to evaluate the model (default is mean squared error).\n",
    "    :return: Dictionary of feature importances for each time step.\n",
    "    \"\"\"\n",
    "    # Store the original model performance\n",
    "    original_performance = metric(y, model.predict(X))\n",
    "\n",
    "    # Number of time steps and features\n",
    "    num_time_steps = X.shape[1]\n",
    "    num_features = X.shape[2]\n",
    "\n",
    "    # Dictionary to hold importances\n",
    "    importances = {}\n",
    "\n",
    "    for time_step in range(num_time_steps):\n",
    "        for feature_index in range(num_features):\n",
    "            # Save original feature\n",
    "            saved_feature = X[:, time_step, feature_index].copy()\n",
    "\n",
    "            # Permute the feature at the specific time step\n",
    "            np.random.shuffle(X[:, time_step, feature_index])\n",
    "\n",
    "            # Calculate new performance\n",
    "            shuffled_performance = metric(y, model.predict(X))\n",
    "\n",
    "            # Restore original feature\n",
    "            X[:, time_step, feature_index] = saved_feature\n",
    "\n",
    "            # Calculate importance\n",
    "            importance = shuffled_performance - original_performance\n",
    "            importances[(time_step, feature_index)] = importance\n",
    "\n",
    "    return importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_importance(model, x, y, mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAILY DEMAND CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos desde el archivo CSV\n",
    "data = pd.read_csv('daily_demand_tr.csv', delimiter=';', skiprows=0)\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# Separar las características de entrada (WD y temp) y la variable de salida (dem)\n",
    "X = data[['WD', 'TEMP']].values\n",
    "y = data['DEM'].values\n",
    "#names_features=data.columns.tolist()\n",
    "print(X)\n",
    "# Normalizar los datos\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "X = scaler_X.fit_transform(X)\n",
    "y = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for RNN\n",
    "N_TIMESTEPS = 8\n",
    "X = reshape_data(X, n_timesteps=N_TIMESTEPS)\n",
    "X = X[:,::-1,:]\n",
    "y = y[N_TIMESTEPS:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only timesteps 1, 2 and 7\n",
    "N_TIMESTEPS_SELECTED=3\n",
    "selected_time_steps = [1, 2, 7]\n",
    "X_selected = X[:, selected_time_steps, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the RNN model and training\n",
    "from keras.optimizers import Adam\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, activation='tanh', return_sequences=False, input_shape=(N_TIMESTEPS_SELECTED, 2)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Step 4: Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Step 5: Train the model\n",
    "model.fit(X_selected, y, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting values y guardar entradas y salidas\n",
    "\n",
    "y_pred = model.predict(X_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the true values line\n",
    "fig.add_trace(go.Scatter(x=list(range(y.shape[0])), y=y.squeeze(), mode='lines', name='True Values'))\n",
    "\n",
    "# Add the predicted values line\n",
    "fig.add_trace(go.Scatter(x=list(range(y.shape[0])), y=y_pred.squeeze(), mode='lines', name='Predicted Values', line=dict(dash='dash')))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='True vs Predicted Values',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Value'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting predictions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y[200:400], lw=3, alpha=0.3, label='Truth')\n",
    "plt.plot(y_pred[200:400], '--', label='Predictions')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivadas Parciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtencion de los pesos\n",
    "model_weights = model.get_weights()\n",
    "W1 =model_weights[0] #input to hidden\n",
    "U1=model_weights[1] #hidden to hidden\n",
    "b1=model_weights[2] #hidden to hidden\n",
    "W2=model_weights[3] #hidden to output\n",
    "b2= model_weights[4] #hidden to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PD = jacobian_rnn(X_selected, W1, U1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_PD_it, sigma_PD_it, PD_it_squared, mean_PD_i, sigma_PD_i, PD_i_squared = compute_sensitivities(PD)\n",
    "\n",
    "print(\"meanPD_it:\", mean_PD_it.shape)\n",
    "print(\"sigma_PD_it:\", sigma_PD_it.shape)\n",
    "print(\"PD_it_squared:\", PD_it_squared.shape)\n",
    "print(\"meanPD_i:\", mean_PD_i.shape)\n",
    "print(\"sigma_PD_i:\", sigma_PD_i.shape)\n",
    "print(\"PD_i_squared:\", PD_i_squared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_partial_derivative_analysis(PD, mean_PD_it, sigma_PD_it, PD_it_squared, [f't-{7}', f't-{2}', f't-{1}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_tabular.RecurrentTabularExplainer(training_data=X_selected, \n",
    "                                                   training_labels=y, \n",
    "                                                   mode='regression',\n",
    "                                                   feature_names=['WD_t-1','TEMP_t-1','WD_t-2','TEMP_t-2','WD_t-7','TEMP_t-7'],\n",
    "                                                   discretize_continuous=False,\n",
    "                                                   class_names=None, \n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "lime_coefs = np.empty((N, N_TIMESTEPS_SELECTED, 2))\n",
    "for p in range(N):\n",
    "    exp = explainer.explain_instance(X_selected[p], model.predict, num_features=lime_coefs.shape[1] * lime_coefs.shape[2], labels=None)\n",
    "    # exp.show_in_notebook()\n",
    "    lime_coefs[p,:,:] = np.array([i[1] for i in exp.as_list()]).reshape(lime_coefs.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_coefs = np.empty((X_selected.shape[0], N_TIMESTEPS_SELECTED, 2))\n",
    "for p in range(X_selected.shape[0]):\n",
    "    exp = explainer.explain_instance(X_selected[p], model.predict, num_features=lime_coefs.shape[1] * lime_coefs.shape[2], labels=None)\n",
    "    # exp.show_in_notebook()\n",
    "    lime_coefs[p,:,:] = np.array([i[1] for i in exp.as_list()]).reshape(lime_coefs.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y[:N], lw=3, alpha=0.3, label='Truth')\n",
    "linetypes=['', '--', '.-', '.']\n",
    "timestep_labels=[1,2,7]\n",
    "for i in range(N_TIMESTEPS_SELECTED * 2):\n",
    "    plt.plot(lime_coefs[:N, i % N_TIMESTEPS_SELECTED, int(i >= N_TIMESTEPS_SELECTED)], linetypes[i % len(linetypes)], label = \"WD t-\" + str(timestep_labels[i]) if i < N_TIMESTEPS_SELECTED else \"TEMP t-\" + str(timestep_labels[i-N_TIMESTEPS_SELECTED]))\n",
    "\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plot the truth\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(N), y=y[:N].reshape(-1,), mode='lines', line=dict(width=3, color='blue', dash='dash'), name='Truth'))\n",
    "\n",
    "# Define linetypes\n",
    "linetypes = ['dash', 'dot', 'dashdot', 'solid']\n",
    "\n",
    "# Plot Lime coefficients\n",
    "for i in range(2 * N_TIMESTEPS_SELECTED):\n",
    "    if i < N_TIMESTEPS_SELECTED:\n",
    "        name = \"WD t-\" + str(timestep_labels[i])\n",
    "        color = 'red'\n",
    "    else:\n",
    "        name = \"TEMP t-\" + str(timestep_labels[i-N_TIMESTEPS_SELECTED])\n",
    "        color = 'green'\n",
    "    fig.add_trace(go.Scatter(x=np.arange(N), y=lime_coefs[:N, i % N_TIMESTEPS_SELECTED, int(i >= N_TIMESTEPS_SELECTED)], mode='lines', line=dict(width=2, color=color, dash=linetypes[i % len(linetypes)]), name=name))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Lime Coefficients',\n",
    "                  xaxis_title='Sample',\n",
    "                  yaxis_title='Coefficient Value',\n",
    "                  legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "                  template='plotly_white')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date ranges for winter (January and February) and summer (July and August)\n",
    "winter_indices = data[(data['DATE'] >= '2008-01-01') & (data['DATE'] <= '2008-02-28')].index\n",
    "summer_indices = data[(data['DATE'] >= '2008-07-01') & (data['DATE'] <= '2008-08-31')].index\n",
    "\n",
    "# Select two weeks from each season (first 14 days of January and July)\n",
    "winter_data_indices = winter_indices[:16]\n",
    "summer_data_indices = summer_indices[:16]\n",
    "\n",
    "X_winter = X_selected[winter_data_indices]\n",
    "y_winter = y[winter_data_indices]\n",
    "X_summer = X_selected[summer_data_indices]\n",
    "y_summer = y[summer_data_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUMMER \n",
    "plt.plot(y_summer, lw=3, alpha=0.3, label='Truth')\n",
    "# Apply LIME to explain the predictions for each day in the selected two weeks of summer\n",
    "lime_coefficients_summer = np.empty((16, N_TIMESTEPS_SELECTED, 2))\n",
    "linetypes=['', '--', '.-', '.']\n",
    "timestep_labels=[1,2,7]\n",
    "for p in range(len(X_summer)):\n",
    "    exp = explainer.explain_instance(X_summer[p].reshape(-1), model.predict, num_features=6)\n",
    "    lime_coefficients_summer[p,:,:] = np.array([i[1]for i in exp.as_list()]).reshape(lime_coefficients_summer.shape[1:])\n",
    "for i in range(N_TIMESTEPS_SELECTED * 2):\n",
    "    plt.plot(lime_coefficients_summer[:N, i % N_TIMESTEPS_SELECTED, int(i >= N_TIMESTEPS_SELECTED)], linetypes[i % len(linetypes)], label = \"WD t-\" + str(timestep_labels[i]) if i < N_TIMESTEPS_SELECTED else \"TEMP t-\" + str(timestep_labels[i-N_TIMESTEPS_SELECTED]))\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WINTER \n",
    "plt.plot(y_winter, lw=3, alpha=0.3, label='Truth')\n",
    "# Apply LIME to explain the predictions for each day in the selected two weeks of winter\n",
    "lime_coefficients_winter = np.empty((16, N_TIMESTEPS_SELECTED, 2))\n",
    "for i in range(len(X_winter)):\n",
    "    exp = explainer.explain_instance(X_winter[i].reshape(-1), model.predict, num_features=6)\n",
    "    lime_coefficients_winter[p,:,:] = np.array([i[1]for i in exp.as_list()]).reshape(lime_coefficients_winter.shape[1:])\n",
    "for i in range(N_TIMESTEPS_SELECTED * 2):\n",
    "    plt.plot(lime_coefficients_winter[:N, i % N_TIMESTEPS_SELECTED, int(i >= N_TIMESTEPS_SELECTED)], linetypes[i % len(linetypes)], label = \"WD t-\" + str(timestep_labels[i]) if i < N_TIMESTEPS_SELECTED else \"TEMP t-\" + str(timestep_labels[i-N_TIMESTEPS_SELECTED]))\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_winter.shape[0]):\n",
    "    exp = explainer.explain_instance(X_winter[i], model.predict, num_features=N_TIMESTEPS_SELECTED*2, labels=None)\n",
    "    exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.DeepExplainer(model, X_selected)\n",
    "\n",
    "shap_values = explainer.shap_values(X_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], np.array(['WD','TEMP']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_of_shap_values = [np.array(values).shape for values in shap_values]\n",
    "\n",
    "print(\"Shapes of shap_values arrays:\", shapes_of_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=[np.array(values).shape for values in shap_values]\n",
    "\n",
    "# Constructing feature names\n",
    "feature_names = []\n",
    "for feature_type in ['WD', 'TEMP']:\n",
    "    for timestep in range(N_TIMESTEPS_SELECTED):\n",
    "        feature_names.append(f'{feature_type}_{timestep}')\n",
    "\n",
    "# Checking if feature_names includes all features represented in shap_values\n",
    "if len(feature_names) != num_features:\n",
    "    print(\"Error: The feature_names arg must include all features represented in shap_values.\")\n",
    "    print(\"Number of features in shap_values:\", num_features)\n",
    "    print(\"Number of elements in feature_names:\", len(feature_names))\n",
    "else:\n",
    "    print(\"Feature names aligned correctly.\")\n",
    "\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "shap_values_array = np.array(shap_values[0])\n",
    "print(\"Shape of shap_values_array:\", shap_values_array.shape)\n",
    "shap_pd_list = []\n",
    "x_pd_list = []\n",
    "names_features=['WD','TEMP']\n",
    "for i in range(shap_values_array.shape[2]):\n",
    "    shap_pd_list.append(pd.DataFrame(shap_values_array[:,:,i], columns=[str(names_features[i]) + '_t' + str(timestep_labels[j]) for j in range(N_TIMESTEPS_SELECTED)]))\n",
    "    x_pd_list.append(pd.DataFrame(X_selected[:,:,i], columns=[str(names_features[i]) + '_t' + str(timestep_labels[j]) for j in range(N_TIMESTEPS_SELECTED)]))\n",
    "shap_pd = pd.concat(shap_pd_list, axis=1)\n",
    "x_pd = pd.concat(x_pd_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_pd.values, x_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_pd.values, x_pd, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SHAP using DeepExplainer\n",
    "explainer = shap.DeepExplainer(model, X_selected)\n",
    "shap_values = explainer.shap_values(X_selected)\n",
    "\n",
    "# Select a specific instance for SHAP plots\n",
    "instance_index = 0\n",
    "\n",
    "# Extract the correct SHAP values and input data for the instance\n",
    "shap_value_instance = shap_values[0][instance_index].flatten()\n",
    "base_value_instance = explainer.expected_value[0]\n",
    "data_instance = X_selected[instance_index].flatten()\n",
    "\n",
    "# Ensure SHAP values and data have the correct dimensions\n",
    "assert shap_value_instance.shape == data_instance.shape, f\"Shapes do not match: {shap_value_instance.shape} vs {data_instance.shape}\"\n",
    "\n",
    "# Waterfall plot for a specific instance\n",
    "shap.waterfall_plot(shap.Explanation(values=shap_value_instance, base_values=base_value_instance, data=data_instance))\n",
    "plt.show()\n",
    "\n",
    "# Reshape for summary plots\n",
    "x_reshaped = X_selected.reshape((X_selected.shape[0], -1))  # Flatten x for summary plot\n",
    "shap_values_reshaped = shap_values[0].reshape((shap_values[0].shape[0], -1))  # Flatten shap values for summary plot\n",
    "\n",
    "# Generate summary plot with bar type\n",
    "shap.summary_plot(shap_values_reshaped, x_reshaped, plot_type=\"bar\")\n",
    "plt.show()\n",
    "names_feat=['WD_t1','TEMP_t1','WD_t2','TEMP_t2','WD_t3','TEMP_t3']\n",
    "# Generate scatter plots for all features\n",
    "num_features = x_reshaped.shape[1]\n",
    "for i in range(num_features):\n",
    "    explanation = shap.Explanation(\n",
    "        values=shap_values_reshaped[:, [i]],\n",
    "        base_values=np.mean(base_value_instance),\n",
    "        data=x_reshaped[:, [i]],\n",
    "        feature_names=[f'Feature {i}']\n",
    "    )\n",
    "    shap.plots.scatter(explanation,color=explanation)\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_all_PDPs(model, X_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_importance(model, X_selected, y, mean_squared_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
